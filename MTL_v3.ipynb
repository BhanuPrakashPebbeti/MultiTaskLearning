{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import cv2\n",
    "from PIL import Image, ImageOps, ImageEnhance, __version__ as PILLOW_VERSION\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, SequentialSampler\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as tvf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from prettytable import PrettyTable\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from pcgrad import PCGrad\n",
    "\n",
    "device = 'cpu'\n",
    "#device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, backbone = 'resnet18', device = 'cuda'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained = True)\n",
    "        self.List = list(self.backbone.children())[:-2]\n",
    "        self.device = device\n",
    "    def forward(self,X):\n",
    "        outputs = []\n",
    "        X = X.to(self.device).float()\n",
    "        for i,layer in enumerate(self.List):\n",
    "            X = layer(X)\n",
    "            if i>1:\n",
    "                outputs.append(X)\n",
    "        return outputs\n",
    " \n",
    "class objdet_Decoder(nn.Module):\n",
    "    '''series of convs ==> final output heatmap'''\n",
    "    def __init__(self, n_classes, stride = 2, device = 'cuda'):\n",
    "        super(objdet_Decoder, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode = 'bilinear')\n",
    "        self.conv1 = nn.Conv2d(512,256,(3,3),padding = 1)  # 16\n",
    "        self.conv2 = nn.Conv2d(256,128,(3,3),padding = 1)  #32\n",
    "        self.conv3 = nn.Conv2d(128,64,(3,3),padding = 1) #64\n",
    "        self.conv4 = nn.Conv2d(64,32,(3,3),padding = 1) #128\n",
    "        self.hmap = nn.Conv2d(32,n_classes,(1,1)) #128\n",
    "        self.regs = nn.Conv2d(32,2,(1,1))\n",
    "        self.w_h_ = nn.Conv2d(32,2,(1,1))\n",
    "        self.to(device)\n",
    "    def forward(self,X):\n",
    "        X = self.upsample(X[-1])\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = self.upsample(X)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = self.upsample(X)\n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = self.upsample(X)\n",
    "        X = F.relu(self.conv4(X))\n",
    "        return [[T.sigmoid(self.hmap(X)), T.sigmoid(self.regs(X)), T.sigmoid(self.w_h_(X))]]\n",
    "        \n",
    "        \n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    " \n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super(DoubleConv,self).__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "        \n",
    "class up(nn.Module):\n",
    "    '''down samling--->double conv'''\n",
    "    def __init__(self,in_channels, out_channels,last_layer=False):\n",
    "        super(up,self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode = 'bilinear')\n",
    "        if last_layer:\n",
    "            self.conv = DoubleConv(in_channels*2,out_channels)\n",
    "        else:\n",
    "            self.conv = DoubleConv(in_channels*3//2,out_channels)   #since we are concatenating \n",
    "    def forward(self,x1,x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        X = T.cat([x1,x2],dim=1)\n",
    "        X = self.conv(X)\n",
    "        return X\n",
    "        \n",
    "class seg_decoder(nn.Module):\n",
    "    def __init__(self, n_classes = 23, device=\"cuda\"):\n",
    "        super(seg_decoder, self).__init__()\n",
    "        \n",
    "        self.up1 = up(512,256)\n",
    "        self.up2 = up(256,128)\n",
    "        self.up3 = up(128,64)\n",
    "        self.up4 = up(64,32,last_layer=True)\n",
    "        self.out_conv = nn.Conv2d(32,n_classes,(3,3),padding=1)\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self,outputs):\n",
    "        X = self.up1(outputs[-1],outputs[-2])\n",
    "        X = self.up2(X,outputs[-3])\n",
    "        X = self.up3(X,outputs[-4])\n",
    "        X = self.up4(X,outputs[-6])\n",
    "        X = self.out_conv(X)\n",
    "        return X\n",
    "     \n",
    "class MTL_Model(nn.Module):\n",
    "    def __init__(self,n_classes = 35,device='cuda'):\n",
    "        super(MTL_Model,self).__init__()\n",
    "        self.encoder = Encoder(device=device)\n",
    "        self.seg_decoder = seg_decoder(n_classes ,device=device)\n",
    "        self.dep_decoder = seg_decoder(n_classes = 1,device=device)\n",
    "        self.obj_decoder = objdet_Decoder(n_classes = 15,device=device)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        outputs = self.encoder(X)\n",
    "        seg_maps = self.seg_decoder(outputs)\n",
    "        depth_maps = self.dep_decoder(outputs)\n",
    "        detection_maps = self.obj_decoder(outputs)\n",
    "        return (seg_maps, T.sigmoid(depth_maps),detection_maps)\n",
    "\n",
    "PALETTE = {\n",
    "    (128, 64,128)  : 0 , #'road' \n",
    "    (250,170,160) : 1 , #'parking'  \n",
    "    ( 81,  0, 81) : 2 ,#drivable fallback\n",
    "    (244, 35,232) : 3 , #sidewalk\n",
    "    (230,150,140) : 4 , #rail track\n",
    "    (152,251,152) : 5 ,#non-drivable fallback\n",
    "    (220, 20, 60) : 6 ,#person\n",
    "    (246, 198, 145) : 7 ,#animal\n",
    "    (255,  0,  0) : 8 , #rider\n",
    "    (  0,  0,230) : 9 ,#motorcycle\n",
    "    (119, 11, 32) : 10 ,  #bicycle\n",
    "    (255, 204, 54) : 11,#autorickshaw\n",
    "    (  0,  0,142) : 12,  #car\n",
    "    (  0,  0, 70) : 13, #truck\n",
    "    (  0, 60,100) : 14,    #bus\n",
    "    (  0,  0, 90) : 15,#caravan\n",
    "    (  0,  0,110) : 16,#trailer\n",
    "    (  0, 80,100) : 17,#train\n",
    "    (136, 143, 153) : 18,#vehicle fallback\n",
    "    (220, 190, 40) : 19,#curb\n",
    "    (102,102,156) : 20,#wall\n",
    "    (190,153,153) : 21,#fence\n",
    "    (180,165,180) : 22,#guard rail\n",
    "    (174, 64, 67) : 23,#billboard\n",
    "    (220,220,  0) : 24,#traffic sign\n",
    "    (250,170, 30) : 25,#traffic light\n",
    "    (153,153,153) : 26,#pole\n",
    "    (169, 187, 214) : 27,#obs-str-bar-fallback\n",
    "    ( 70, 70, 70) : 28,#building\n",
    "    (150,100,100) : 29,#bridge\n",
    "    (150,120, 90) : 30,#tunnel\n",
    "    (107,142, 35) : 31,#vegetation\n",
    "    ( 70,130,180) : 32,#sky\n",
    "    (169, 187, 214) : 33,#fallback background\n",
    "    (  0,  0,  0) : 34#unlabeled\n",
    "}\n",
    "\n",
    "def convert_from_color_segmentation(arr_3d):\n",
    "    arr_3d = np.array(arr_3d)\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "    palette = PALETTE\n",
    "    for i in range(0, arr_3d.shape[0]):\n",
    "        for j in range(0, arr_3d.shape[1]):\n",
    "            key = (arr_3d[i, j, 2], arr_3d[i, j, 1], arr_3d[i, j, 0])\n",
    "            arr_2d[i, j] = palette.get(key,34) # default value if key was not found is 0\n",
    "\n",
    "    return arr_2d\n",
    "\n",
    "def labels_to_cityscapes_palette(array):\n",
    "    result = np.zeros((array.shape[0], array.shape[1], 3))\n",
    "    for value, key in PALETTE.items():\n",
    "        result[np.where(array == key)] = (value[2],value[1],value[0])\n",
    "    return result/255\n",
    "\n",
    "def to_one_hot(mask, n_classes=35):\n",
    "    one_hot = np.zeros((mask.shape[0], mask.shape[1], n_classes))\n",
    "    for i, unique_value in enumerate(np.unique(mask)):\n",
    "        one_hot[:, :, unique_value][mask == unique_value] = 1\n",
    "    return one_hot    \n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=4, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = T.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return T.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "    \n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, eps=1e-7):\n",
    "        inputs = F.softmax(inputs, dim=1)\n",
    "        targets = targets.type(inputs.type())\n",
    "        intersection = T.sum(inputs * targets, (0, 2, 3))\n",
    "        cardinality = T.sum(inputs + targets, (0, 2, 3))\n",
    "        dice_loss = (2. * intersection / (cardinality + eps)).mean()\n",
    "        return (1 - dice_loss)\n",
    "    \n",
    "class DiceFocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceFocalLoss, self).__init__()\n",
    "        self.criterion = FocalLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        targets_ = T.argmax(targets, dim=1)\n",
    "        floss = self.criterion(inputs, targets_.long())\n",
    "        dice_loss = self.dice_loss(inputs,targets)\n",
    "        Dice_BCE = floss + dice_loss\n",
    "        return Dice_BCE\n",
    "\n",
    "class DepthLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DepthLoss, self).__init__()\n",
    "    def im_gradient_loss(self,d_batch, n_pixels):\n",
    "        a = T.Tensor([[[[1, 0, -1],\n",
    "                            [2, 0, -2],\n",
    "                            [1, 0, -1]]]])\n",
    "                      \n",
    "        b = T.Tensor([[[[1, 2, 1],\n",
    "                            [0, 0, 0],\n",
    "                            [-1, -2, -1]]]])\n",
    "        \n",
    "        a = a.to(device)\n",
    "        b = b.to(device)\n",
    "\n",
    "        G_x = F.conv2d(d_batch, a, padding=1).to(device)\n",
    "        G_y = F.conv2d(d_batch, b, padding=1).to(device)\n",
    "        \n",
    "        G = T.pow(G_x,2)+ T.pow(G_y,2)\n",
    "    \n",
    "        return G.view(-1, n_pixels).mean(dim=1).mean()\n",
    "\n",
    "    def forward(self,preds, actual_depth):\n",
    "        \n",
    "        n_pixels = actual_depth.shape[2]*actual_depth.shape[3]\n",
    "        preds = preds*1000\n",
    "        preds[preds<=0] = 0.00001\n",
    "        actual_depth[actual_depth==0] = 0.00001\n",
    "        d = T.log(preds) - T.log(actual_depth)\n",
    "        grad_loss_term = self.im_gradient_loss(d, n_pixels)\n",
    "        term_1 = T.pow(d.view(-1, n_pixels),2).mean(dim=1).mean() #pixel wise mean, then batch sum\n",
    "        term_2 = (T.pow(d.view(-1, n_pixels).sum(dim=1),2)/(2*(n_pixels**2))).mean()\n",
    "        loss1 = term_1 - term_2 + grad_loss_term\n",
    "        loss2 = F.mse_loss(preds,actual_depth,reduction='mean')\n",
    "        return loss1 + loss2\n",
    "    \n",
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DetectionLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,obj, hmap, regs, w_h_):\n",
    "        regs = [self._tranpose_and_gather_feature(r, obj['inds']) for r in regs]\n",
    "        w_h_ = [self._tranpose_and_gather_feature(r, obj['inds']) for r in w_h_]\n",
    "        hmap_loss = self._neg_loss(hmap, obj['hmap'])\n",
    "        reg_loss = self._reg_loss(regs, obj['regs'], obj['ind_masks'])\n",
    "        w_h_loss = self._reg_loss(w_h_, obj['w_h_'], obj['ind_masks'])\n",
    "        loss =  0.5*hmap_loss +  reg_loss +  w_h_loss \n",
    "        return loss \n",
    "    \n",
    "    def _neg_loss(self,preds, targets):\n",
    "        pos_inds = targets.eq(1).float()\n",
    "        neg_inds = targets.lt(1).float()\n",
    "        neg_weights = T.pow(1 - targets, 4)\n",
    "        loss = 0\n",
    "        for pred in preds:\n",
    "            pred = T.clamp(T.sigmoid(pred), min=1e-4, max=1 - 1e-4)\n",
    "            pos_loss = T.log(pred) * T.pow(1 - pred, 2) * pos_inds\n",
    "            neg_loss = T.log(1 - pred) * T.pow(pred, 2) * neg_weights * neg_inds\n",
    "\n",
    "            num_pos = pos_inds.float().sum()\n",
    "            pos_loss = pos_loss.sum()\n",
    "            neg_loss = neg_loss.sum()\n",
    "\n",
    "            if num_pos == 0:\n",
    "                loss = loss - neg_loss\n",
    "            else:\n",
    "                loss = loss - (pos_loss + neg_loss) / num_pos\n",
    "        return loss / len(preds)\n",
    "    \n",
    "    def _reg_loss(self,regs, gt_regs, mask):\n",
    "        mask = mask[:, :, None].expand_as(gt_regs).float()\n",
    "        loss = sum(F.l1_loss(r * mask, gt_regs * mask, reduction='sum') / (mask.sum() + 1e-4) for r in regs)\n",
    "        return loss / len(regs)\n",
    "    \n",
    "    def _gather_feature(self,feat, ind, mask=None):\n",
    "        dim = feat.size(2)\n",
    "        ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n",
    "        feat = feat.gather(1, ind)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(2).expand_as(feat)\n",
    "            feat = feat[mask]\n",
    "            feat = feat.view(-1, dim)\n",
    "        return feat\n",
    "    \n",
    "    def _tranpose_and_gather_feature(self,feat, ind):\n",
    "        feat = feat.permute(0, 2, 3, 1).contiguous()\n",
    "        feat = feat.view(feat.size(0), -1, feat.size(3))\n",
    "        feat = _gather_feature(feat, ind)\n",
    "        return feat\n",
    "\n",
    "def _neg_loss(preds, targets):\n",
    "    pos_inds = targets.eq(1).float()\n",
    "    neg_inds = targets.lt(1).float()\n",
    "\n",
    "    neg_weights = T.pow(1 - targets, 4)\n",
    "\n",
    "    loss = 0\n",
    "    for pred in preds:\n",
    "        pred = T.clamp(pred, min=1e-4, max=1 - 1e-4)\n",
    "        pos_loss = T.log(pred) * T.pow(1 - pred, 2) * pos_inds\n",
    "        neg_loss = T.log(1 - pred) * T.pow(pred, 2) * neg_weights * neg_inds\n",
    "\n",
    "        num_pos = pos_inds.float().sum()\n",
    "        pos_loss = pos_loss.sum()\n",
    "        neg_loss = neg_loss.sum()\n",
    "        if num_pos == 0:\n",
    "            loss = loss - neg_loss\n",
    "        else:\n",
    "            loss = loss - (pos_loss + neg_loss) / num_pos\n",
    "    return loss / len(preds)\n",
    "\n",
    "\n",
    "def _reg_loss(regs, gt_regs, mask):\n",
    "    mask = mask[:, :, None].expand_as(gt_regs).float()\n",
    "    loss = sum(F.l1_loss(r * mask, gt_regs * mask, reduction='sum') / (mask.sum() + 1e-4) for r in regs)\n",
    "    return loss / len(regs)\n",
    "\n",
    "input_size_x,input_size_y = (640, 480)\n",
    "MODEL_SCALE = 2\n",
    "\n",
    "def _gather_feature(feat, ind, mask=None):\n",
    "    dim = feat.size(2)\n",
    "    ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n",
    "    feat = feat.gather(1, ind)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(2).expand_as(feat)\n",
    "        feat = feat[mask]\n",
    "        feat = feat.view(-1, dim)\n",
    "    return feat\n",
    "\n",
    "def _tranpose_and_gather_feature(feat, ind):\n",
    "    feat = feat.permute(0, 2, 3, 1).contiguous()\n",
    "    feat = feat.view(feat.size(0), -1, feat.size(3))\n",
    "    feat = _gather_feature(feat, ind)\n",
    "    return feat\n",
    "\n",
    "def gaussian2D(shape, sigma=1):\n",
    "    m, n = [(ss - 1.) / 2. for ss in shape]\n",
    "    y, x = np.ogrid[-m:m + 1, -n:n + 1]\n",
    "\n",
    "    h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))\n",
    "    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n",
    "    return h\n",
    "\n",
    "def gaussian_radius(det_size, min_overlap=0.7):\n",
    "    height, width = det_size\n",
    "\n",
    "    a1 = 1\n",
    "    b1 = (height + width)\n",
    "    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n",
    "    sq1 = np.sqrt(b1 ** 2 - 4 * a1 * c1)\n",
    "    r1 = (b1 - sq1) / (2 * a1)\n",
    "\n",
    "    a2 = 4\n",
    "    b2 = 2 * (height + width)\n",
    "    c2 = (1 - min_overlap) * width * height\n",
    "    sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2)\n",
    "    r2 = (b2 - sq2) / (2 * a2)\n",
    "\n",
    "    a3 = 4 * min_overlap\n",
    "    b3 = -2 * min_overlap * (height + width)\n",
    "    c3 = (min_overlap - 1) * width * height\n",
    "    sq3 = np.sqrt(b3 ** 2 - 4 * a3 * c3)\n",
    "    r3 = (b3 + sq3) / 2\n",
    "    return min(r1, r2, r3)\n",
    "\n",
    "def draw_umich_gaussian(heatmap, center, radius, k=1):\n",
    "    diameter = 2 * radius + 1\n",
    "    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)\n",
    "\n",
    "    x, y = int(center[0]), int(center[1])\n",
    "\n",
    "    height, width = heatmap.shape[0:2]\n",
    "\n",
    "    left, right = min(x, radius), min(width - x, radius + 1)\n",
    "    top, bottom = min(y, radius), min(height - y, radius + 1)\n",
    "\n",
    "    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n",
    "    masked_gaussian = gaussian[radius - top:radius + bottom, radius - left:radius + right]\n",
    "    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:  # TODO debug\n",
    "        np.maximum(masked_heatmap, masked_gaussian * k, out=masked_heatmap)\n",
    "    return heatmap\n",
    "\n",
    "def convert(obj,width,height):\n",
    "    x_scale = 640 / width\n",
    "    y_scale = 480 / height\n",
    "    x_c = int(np.round(((obj[0]+obj[2])/2)*x_scale))\n",
    "    y_c = int(np.round(((obj[1]+obj[3])/2)*y_scale))\n",
    "    w = int(np.round((obj[2]-obj[0])*x_scale))\n",
    "    h = int(np.round((obj[3]-obj[1])*y_scale))\n",
    "    box = [x_c,y_c,w,h]\n",
    "    return box\n",
    "\n",
    "def make_hm_regr(target,width,height,num_classes = 15,input_size_x = 640,input_size_y = 480,MODEL_SCALE=2,max_objs=240,gaussian_iou = 0.7):\n",
    "    hmap = np.zeros((num_classes, input_size_y//MODEL_SCALE, input_size_x//MODEL_SCALE), dtype=np.float32)\n",
    "    w_h_ = np.zeros((max_objs, 2), dtype=np.float32)\n",
    "    regs = np.zeros((max_objs, 2), dtype=np.float32)\n",
    "    inds = np.zeros((max_objs,), dtype=np.int64)\n",
    "    ind_masks = np.zeros((max_objs,), dtype=np.uint8)\n",
    "    boxes = literal_eval(target[\"bbox\"])\n",
    "    classes = {\"bicycle\":0,\"bus\":1,\"traffic sign\":2,\"train\":3,\"motorcycle\":4,\"car\":5,\"traffic light\":6,\"person\":7,\"vehicle fallback\":8,\"truck\":9,\"autorickshaw\":10,\"animal\":11,\"caravan\":12,\"rider\":13,\"trailer\":14}\n",
    "\n",
    "    for i,a in enumerate(boxes):\n",
    "        box_ = a[\"bbox\"]\n",
    "        box = convert(box_,width,height)\n",
    "        if (box[0]>640) or (box[1]>480):\n",
    "            continue\n",
    "        center = np.array([(box[0]),(box[1])], dtype=np.float32)\n",
    "        obj_c = np.array([(box[0]//MODEL_SCALE),(box[1]//MODEL_SCALE)], dtype=np.float32)\n",
    "        obj_c_int = obj_c.astype(np.int32)\n",
    "        h = box[3]\n",
    "        w = box[2]\n",
    "        if h > 0 and w > 0:\n",
    "            radius = max(0, int(gaussian_radius((math.ceil(h), math.ceil(w)), gaussian_iou)))\n",
    "            hmap[classes[a[\"label\"]],:,:] = draw_umich_gaussian(hmap[classes[a[\"label\"]],:,:], obj_c_int, radius)   \n",
    "            w_h_[i] =  w/input_size_x, h/input_size_y\n",
    "            regs[i] = center - (obj_c_int*MODEL_SCALE)\n",
    "            inds[i] = ((obj_c_int[1]) * (input_size_x//MODEL_SCALE)) + (obj_c_int[0])\n",
    "            ind_masks[i] = 1\n",
    "    return {'hmap': hmap, 'w_h_': w_h_, 'regs': regs, 'inds': inds, 'ind_masks': ind_masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_Model(nn.Module):\n",
    "    def __init__(self,n_classes = 35,device='cuda'):\n",
    "        super(MTL_Model,self).__init__()\n",
    "        self.encoder = Encoder(device=device)\n",
    "        self.seg_decoder = seg_decoder(n_classes ,device=device)\n",
    "        self.dep_decoder = seg_decoder(n_classes = 1,device=device)\n",
    "        self.obj_decoder = objdet_Decoder(n_classes = 15,device=device)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        outputs = self.encoder(X)\n",
    "        for i in outputs:\n",
    "            print(i.shape)\n",
    "        seg_maps = self.seg_decoder(outputs)\n",
    "        depth_maps = self.dep_decoder(outputs)\n",
    "        detection_maps = self.obj_decoder(outputs)\n",
    "        return (seg_maps, T.sigmoid(depth_maps),detection_maps)\n",
    "\n",
    "x = T.rand(2,3,480,640)\n",
    "x.shape\n",
    "model = MTL_Model(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b170007ec/.conda/envs/AIR/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 240, 320])\n",
      "torch.Size([2, 64, 120, 160])\n",
      "torch.Size([2, 64, 120, 160])\n",
      "torch.Size([2, 128, 60, 80])\n",
      "torch.Size([2, 256, 30, 40])\n",
      "torch.Size([2, 512, 15, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b170007ec/.conda/envs/AIR/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    }
   ],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c981bc5a0e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMTL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m240\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class MTL(Dataset):\n",
    "    def __init__(self, filename=None, input_size=(640, 480), output_size=(320, 240), n_classes=15):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.n_classes = n_classes\n",
    "        self.max_objs = 240\n",
    "        self.gaussian_iou = 0.7\n",
    "        self.dataset = pd.read_csv(self.filename)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size_x = self.input_size[0]\n",
    "        self.input_size_y = self.input_size[1]\n",
    "        self.MODEL_SCALE = self.input_size[0]//self.output_size[0]\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.resize1 = transforms.Compose([transforms.Resize(self.input_size)])\n",
    "        self.resize2 = transforms.Compose([transforms.Resize(self.output_size)])\n",
    "\n",
    "\n",
    "    def __len__(self): return len(self.dataset)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        target = self.dataset.iloc[idx]\n",
    "        rgb_image = cv2.imread(target[\"Path\"])\n",
    "        height, width, channels = rgb_image.shape\n",
    "        rgb_image = cv2.resize(rgb_image,self.input_size)\n",
    "        obj = make_hm_regr(target,width,height,self.n_classes,self.input_size_x,self.input_size_y,self.MODEL_SCALE,self.max_objs,self.gaussian_iou)\n",
    "        seg_mask = np.load(target[\"Seg_Path\"])\n",
    "        depth_image = np.load(target[\"Depth_path\"])\n",
    "        depth_image = cv2.resize(depth_image,self.output_size)\n",
    "        seg_mask = cv2.resize(seg_mask,self.output_size)\n",
    "        one_hot_segmask = to_one_hot(seg_mask)\n",
    "        if preprocess:\n",
    "            rgb_image = self.preprocess(np.array(rgb_image))\n",
    "            one_hot_segmask = transforms.ToTensor()(np.array(one_hot_segmask))\n",
    "            depth_image = transforms.ToTensor()(np.array(depth_image))\n",
    "        else:\n",
    "            rgb_image = transforms.ToTensor()(np.array(rgb_image))\n",
    "            one_hot_segmask = transforms.ToTensor()(np.array(one_hot_segmask))\n",
    "            depth_image = transforms.ToTensor()(np.array(depth_image))\n",
    "            seg_mask = transforms.ToTensor()(np.array(seg_mask))\n",
    "        return (rgb_image,seg_mask,one_hot_segmask,depth_image, obj)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)\n",
    "    \n",
    "model = MTL_Model(device = device)\n",
    "print(device)\n",
    "#model.load_state_dict(T.load(\"/home/b170007ec/Programs/MTL/DSD_MTL/Models/model-1.713353544473648.pth\",map_location=T.device('cpu')))\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(pytorch_total_params)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(pytorch_total_params)\n",
    "    print()\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "#count_parameters(model)\n",
    "train_dataloader = MTL(\"/home/b170007ec/Programs/MTL/DSD_MTL/Dataset/train_dataset.csv\")\n",
    "print(\"Train :\",train_dataloader.__len__())\n",
    "val_dataloader = MTL(\"/home/b170007ec/Programs/MTL/DSD_MTL/Dataset/val_dataset.csv\")\n",
    "print(\"Val :\",val_dataloader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diceloss = DiceFocalLoss()\n",
    "depthloss = DepthLoss()\n",
    "\n",
    "def loss_fn(y_pred, y_true, obj, hmap, regs, w_h_):\n",
    "    (pred_seg, pred_depth) = y_pred\n",
    "    (true_seg, true_depth) = y_true\n",
    "    dice = diceloss(pred_seg, true_seg)\n",
    "    depth = depthloss(pred_depth, true_depth)\n",
    "    #detect = detectionloss(obj, hmap, regs, w_h_)\n",
    "    regs = [_tranpose_and_gather_feature(r, obj['inds']) for r in regs]\n",
    "    w_h_ = [_tranpose_and_gather_feature(r, obj['inds']) for r in w_h_]\n",
    "    hmap_loss = _neg_loss(hmap, obj['hmap'])\n",
    "    reg_loss = _reg_loss(regs, obj['regs'], obj['ind_masks'])\n",
    "    w_h_loss = _reg_loss(w_h_, obj['w_h_'], obj['ind_masks'])\n",
    "    detect =  0.5*hmap_loss +  reg_loss +  w_h_loss \n",
    "    return dice+depth+detect, dice, depth, detect \n",
    "\n",
    "@T.no_grad()\n",
    "def validation(model, loader, loss_fn):\n",
    "    vlosses = []\n",
    "    dice_vloss = []\n",
    "    depth_vloss = []\n",
    "    detect_vloss = []\n",
    "    model.eval()\n",
    "    for rgb,seg_mask,seg,depth,obj in loader:\n",
    "        rgb,seg,depth = rgb.to(device), seg.to(device), depth.to(device)\n",
    "        obj['hmap'], obj['w_h_'], obj['regs'], obj['inds'], obj['ind_masks'] = obj['hmap'].to(device), obj['w_h_'].to(device), obj['regs'].to(device), obj['inds'].to(device), obj['ind_masks'].to(device)\n",
    "        y_pred = model(rgb)\n",
    "        hmap, regs, w_h_ = zip(*y_pred[2])\n",
    "        y_true = (seg,depth)\n",
    "        loss, v_dice, v_depth, v_detect = loss_fn((y_pred[0],y_pred[1]), y_true, obj, hmap, regs, w_h_)\n",
    "        dice_vloss.append(v_dice.item())\n",
    "        depth_vloss.append(v_depth.item())\n",
    "        detect_vloss.append(v_detect.item())\n",
    "        vlosses.append(loss.item())\n",
    "    return np.array(vlosses).mean(), np.array(dice_vloss).mean(), np.array(depth_vloss).mean(), np.array(detect_vloss).mean()\n",
    "\n",
    "batch_size = 25\n",
    "EPOCHES = 250\n",
    "\n",
    "train_loader = DataLoader(train_dataloader,batch_size=batch_size,shuffle=False, num_workers=0, sampler=SubsetRandomSampler(list(range(train_dataloader.__len__()))),\n",
    "                             drop_last=False)\n",
    "val_loader = DataLoader(val_dataloader,batch_size=batch_size,shuffle=False,\n",
    "                              num_workers=0,\n",
    "                              sampler=SubsetRandomSampler(list(range(len(val_dataloader.dataset)))),\n",
    "                             drop_last=False)\n",
    "raw_line0 = r'''Epoch[{}]    |    Lr:{}'''\n",
    "raw_line1 = r'''Train Loss:[SEG:{}+DEPTH:{}+DETECT:{}] | Val Loss:[SEG:{}+DEPTH:{}+DETECT:{}]'''\n",
    "raw_line3 = r'''TOTAL Train loss: {}  |  TOTAL Val loss: {}  |  Time:{:.1f} min '''\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = PCGrad(T.optim.Adam([\n",
    "                {'params': model.parameters()}]\n",
    "                , lr=0.0001))\n",
    "scheduler = T.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=10,verbose=True)\n",
    "best_loss = None\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHES+1):\n",
    "    losses = []\n",
    "    dice_loss = []\n",
    "    depth_loss = []\n",
    "    detect_loss = []\n",
    "    start_time = time.time()\n",
    "    t = tqdm(train_loader)\n",
    "    model.train()\n",
    "    for i,(rgb,seg_mask,seg,depth,obj) in enumerate(t):\n",
    "        rgb,seg,depth = rgb.to(device), seg.to(device), depth.to(device)\n",
    "        obj['hmap'], obj['w_h_'], obj['regs'], obj['inds'], obj['ind_masks'] = obj['hmap'].to(device), obj['w_h_'].to(device), obj['regs'].to(device), obj['inds'].to(device), obj['ind_masks'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(rgb)\n",
    "        hmap, regs, w_h_ = zip(*y_pred[2])\n",
    "        loss, dice, depth, detect = loss_fn((y_pred[0],y_pred[1]), (seg,depth), obj, hmap, regs, w_h_)\n",
    "        losses = [dice,depth,detect]\n",
    "#         loss.backward()\n",
    "        optimizer.pc_backward(losses) \n",
    "        optimizer.step()\n",
    "        dice_loss.append(dice.item())\n",
    "        depth_loss.append(depth.item())\n",
    "        detect_loss.append(detect.item())\n",
    "        losses.append(loss.item())\n",
    "    vloss, vdice, vdepth, vdetect = validation(model, val_loader, loss_fn)\n",
    "    print(raw_line0.format(epoch,optimizer.param_groups[0][\"lr\"]))\n",
    "    print(raw_line1.format(np.array(dice_loss).mean(),np.array(depth_loss).mean(),np.array(detect_loss).mean(),vdice,vdepth,vdetect))\n",
    "    print(raw_line3.format(np.array(losses).mean(),vloss,(time.time()-start_time)/60**1))\n",
    "    \n",
    "    if best_loss == None:\n",
    "        best_loss = vloss\n",
    "        T.save(model.state_dict(), '/home/b170007ec/Programs/MTL/VQ_MTL/Models/model_dsd-{}.pth'.format(best_loss))\n",
    "        print(\"saving model ..\")\n",
    "    if vloss < best_loss:\n",
    "        best_loss = vloss\n",
    "        T.save(model.state_dict(), '/home/b170007ec/Programs/MTL/VQ_MTL/Models/model_dsd-{}.pth'.format(best_loss))\n",
    "        print(\"saving model ..\")\n",
    "    scheduler.step(vloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(T.load(\"/home/b170007ec/Programs/MTL/DSD_MTL/MTL_V3/model_v3-1.8697537092062144.pth\",map_location=T.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showbox(img, hm, off,regr,box_=None):\n",
    "    sample = cv2.resize(img,(640, 480))\n",
    "    boxes = ctdet_decode(hm,off,regr)\n",
    "    \n",
    "    classes = {0:\"bicycle\",1:\"bus\",2:\"traffic sign\",3:\"train\",4:\"motorcycle\",5:\"car\",6:\"traffic light\",7:\"person\",8:\"vehicle fallback\",9:\"truck\",10:\"autorickshaw\",11:\"animal\",12:\"caravan\",13:\"rider\",14:\"trailer\"}\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontScale = 0.5\n",
    "    thickness = 1\n",
    "    color =(250, 0, 0)\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,\n",
    "                      (int(box[0]-(box[2]/2)), int(box[1]-(box[3]/2))),\n",
    "                      (int(box[0]+(box[2]/2)), int(box[1]+(box[3]/2))),\n",
    "                      color, 2)\n",
    "    return sample\n",
    "\n",
    "def _nms(heat, kernel=7):\n",
    "    hmax = F.max_pool2d(heat, kernel, stride=1, padding=(kernel - 1) // 2)\n",
    "    keep = (hmax == heat).float()\n",
    "    return heat * keep\n",
    "\n",
    "\n",
    "def _topk(scores, K=40, threshold=0.2):\n",
    "    batch, cat, height, width = scores.size()\n",
    "\n",
    "    topk_scores, topk_inds = T.topk(scores.view(batch, cat, -1), K)\n",
    "\n",
    "    topk_inds = topk_inds % (height * width)\n",
    "    topk_ys = (topk_inds / width).int().float()\n",
    "    topk_xs = (topk_inds % width).int().float()\n",
    "\n",
    "    topk_score, topk_ind = T.topk(topk_scores.view(batch, -1), K)\n",
    "    topk_clses = (topk_ind / K).int()\n",
    "    topk_inds = _gather_feature(topk_inds.view(batch, -1, 1), topk_ind).view(batch, K)\n",
    "    topk_ys = _gather_feature(topk_ys.view(batch, -1, 1), topk_ind).view(batch, K)\n",
    "    topk_xs = _gather_feature(topk_xs.view(batch, -1, 1), topk_ind).view(batch, K)\n",
    "    mask = T.where(topk_score>threshold, True, False)\n",
    "    return topk_score[:,mask[0]], topk_inds[:,mask[0]], topk_clses[:,mask[0]], topk_ys[:,mask[0]], topk_xs[:,mask[0]], len(topk_score[:,mask[0]][0])\n",
    "\n",
    "\n",
    "def ctdet_decode(hmap, regs, w_h_, K=40):\n",
    "    batch, cat, height, width = hmap.shape\n",
    "    batch = 1\n",
    "    input_size_x = 640\n",
    "    input_size_y = 480\n",
    "    hmap = _nms(hmap)  # perform nms on heatmaps\n",
    "\n",
    "    scores, inds, clses, ys, xs, M = _topk(hmap, K=K)\n",
    "    regs = _tranpose_and_gather_feature(regs, inds)\n",
    "    regs = regs.view(batch, M, 2)\n",
    "    xs = xs.view(batch, M, 1)*MODEL_SCALE + regs[:, :, 0:1]\n",
    "    ys = ys.view(batch, M, 1)*MODEL_SCALE + regs[:, :, 1:2]\n",
    "    w_h_ = _tranpose_and_gather_feature(w_h_, inds)\n",
    "    w_h_ = w_h_.view(batch, M, 2)\n",
    "\n",
    "    clses = clses.view(batch, M, 1).float()\n",
    "    scores = scores.view(batch, M, 1)\n",
    "    bboxes = T.cat([xs ,ys  ,w_h_[..., 0:1]*input_size_x ,w_h_[..., 1:2]*input_size_y ], dim=2)\n",
    "    detections = T.cat([bboxes, scores, clses], dim=2)\n",
    "    return detections.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL_TEST_DETECTOR(Dataset):\n",
    "    def __init__(self, filename=None, input_size=(640, 480), output_size=(320, 240)):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.dataset = pd.read_csv(self.filename)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size_x = self.input_size[0]\n",
    "        self.input_size_y = self.input_size[1]\n",
    "        self.MODEL_SCALE = self.input_size[0]//self.output_size[0]\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.resize1 = transforms.Compose([transforms.Resize(self.input_size)])\n",
    "        self.resize2 = transforms.Compose([transforms.Resize(self.output_size)])\n",
    "\n",
    "\n",
    "    def __len__(self): return len(self.dataset)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        target = self.dataset.iloc[idx]\n",
    "        rgb_image = cv2.imread(target[\"Path\"])\n",
    "        height, width, channels = rgb_image.shape\n",
    "        rgb_image = cv2.resize(rgb_image,self.input_size)\n",
    "        boxes = literal_eval(target[\"bbox\"])\n",
    "        b = []\n",
    "        for i,a in enumerate(boxes):\n",
    "            box_ = a[\"bbox\"]\n",
    "            box = convert(box_,width,height)\n",
    "            b.append(box)\n",
    "        if preprocess:\n",
    "            rgb_image = self.preprocess(np.array(rgb_image))\n",
    "        else:\n",
    "            rgb_image = transforms.ToTensor()(np.array(rgb_image))\n",
    "        return (rgb_image, b)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = MTL_TEST_DETECTOR(\"/home/b170007ec/Programs/MTL/DSD_MTL/Dataset/val_dataset.csv\")\n",
    "# model.eval()\n",
    "# device = T.device(\"cpu\")\n",
    "# j=10\n",
    "# for i in range(j,j+30,5):\n",
    "#     matrix = []\n",
    "#     for a in range(5):\n",
    "#         rgb ,box = test_dataloader[i+a]\n",
    "#         rgb_raw,_ = test_dataloader.raw(i+a)\n",
    "#         rgb_raw = rgb_raw.permute(1,2,0)\n",
    "#         rgb = T.unsqueeze(rgb, 0)\n",
    "#         rgb = rgb.to(device)\n",
    "#         y_pred = model(rgb)\n",
    "#         y_pred_ = F.softmax(y_pred[0],dim=1)\n",
    "#         pseg = T.squeeze(y_pred_,0)\n",
    "#         pseg = T.argmax(pseg, dim=0)\n",
    "#         pdepth = T.squeeze(y_pred[1],0)\n",
    "#         pdepth = pdepth.permute(1,2,0)\n",
    "#         pdepth = pdepth.reshape(240,320)\n",
    "#         hmap, regs, w_h_ = zip(*y_pred[2])\n",
    "#         d = showbox(rgb_raw.numpy(), hmap[0].detach(), regs[0].detach(),w_h_[0].detach())\n",
    "#         matrix.append([rgb_raw.numpy(),labels_to_cityscapes_palette(pseg.cpu().detach().numpy()),pdepth.cpu().detach().numpy(),d])\n",
    "    \n",
    "#     fig, ax = plt.subplots(5, 4,figsize=(40,40))\n",
    "#     for k in range(5):\n",
    "#         for j in range(4):\n",
    "#             ax[k][j].imshow(matrix[k][j])\n",
    "#             ax[k][j].set_xticks([])\n",
    "#             ax[k][j].set_yticks([])\n",
    "#     plt.savefig(\"END_TO_END_MTL_{}\".format(i))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/b170007ec/.conda/envs/AIR/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/b170007ec/.conda/envs/AIR/lib/python3.7/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "100%|██████████| 100/100 [18:28<00:00, 11.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation :     IOU :0.9793402734016757\n",
      "                        Pixcel Acc :0.943042317708333\n",
      "Depth Estimation : A1 :0.8524420572916663 \n",
      "                   A2 :0.9671630208333329 \n",
      "                   A3 :0.9899037760416664 \n",
      "                   ABS_REL :0.1191964515298605 \n",
      "                   RMSE :0.030864654602482916 \n",
      "                   LOG_10 :0.05106191769242287\n",
      "VOC PASCAL mAP in all points: 0.2563420832157135\n",
      "Detection IOU : tensor([0.7261])\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def compute_errors(gt, pred):\n",
    "    pred *= 1000\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25).mean()\n",
    "    a2 = (thresh < (1.25 ** 2)).mean()\n",
    "    a3 = (thresh < (1.25 ** 3)).mean()\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "    log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\n",
    "    return a1, a2, a3, abs_rel, rmse, log_10\n",
    "\n",
    "def check_size(eval_segm, gt_segm):\n",
    "    h_e, w_e = segm_size(eval_segm)\n",
    "    h_g, w_g = segm_size(gt_segm)\n",
    "\n",
    "    if (h_e != h_g) or (w_e != w_g):\n",
    "        raise EvalSegErr(\"DiffDim: Different dimensions of matrices!\")\n",
    "\n",
    "'''\n",
    "Exceptions\n",
    "'''\n",
    "class EvalSegErr(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "    \n",
    "def extract_masks(segm, cl, n_cl):\n",
    "    h, w  = segm_size(segm)\n",
    "    masks = np.zeros((n_cl, h, w))\n",
    "\n",
    "    for i, c in enumerate(cl):\n",
    "        masks[i, :, :] = segm == c\n",
    "\n",
    "    return masks\n",
    "\n",
    "def segm_size(segm):\n",
    "    try:\n",
    "        height = segm.shape[0]\n",
    "        width  = segm.shape[1]\n",
    "    except IndexError:\n",
    "        raise\n",
    "\n",
    "    return height, width\n",
    "\n",
    "def extract_both_masks(eval_segm, gt_segm, cl, n_cl):\n",
    "    eval_mask = extract_masks(eval_segm, cl, n_cl)\n",
    "    gt_mask   = extract_masks(gt_segm, cl, n_cl)\n",
    "\n",
    "    return eval_mask, gt_mask\n",
    "\n",
    "def extract_classes(segm):\n",
    "    cl = np.unique(segm)\n",
    "    n_cl = len(cl)\n",
    "\n",
    "    return cl, n_cl\n",
    "\n",
    "\n",
    "def IOU_SCORE(eval_segm, gt_segm):\n",
    "    intersection = np.logical_and(gt_segm, eval_segm)\n",
    "    union = np.logical_or(gt_segm, eval_segm)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    \n",
    "    check_size(eval_segm, gt_segm)\n",
    "\n",
    "    cl, n_cl = extract_classes(gt_segm)\n",
    "    eval_mask, gt_mask = extract_both_masks(eval_segm, gt_segm, cl, n_cl)\n",
    "    \n",
    "    sum_n_ii = 0\n",
    "    sum_t_i  = 0\n",
    "\n",
    "    for i, c in enumerate(cl):\n",
    "        curr_eval_mask = eval_mask[i, :, :]\n",
    "        curr_gt_mask = gt_mask[i, :, :]\n",
    "\n",
    "        sum_n_ii += np.sum(np.logical_and(curr_eval_mask, curr_gt_mask))\n",
    "        sum_t_i  += np.sum(curr_gt_mask)\n",
    " \n",
    "    if (sum_t_i == 0):\n",
    "        pixel_accuracy_ = 0\n",
    "    else:\n",
    "        pixel_accuracy_ = sum_n_ii / sum_t_i\n",
    "    \n",
    "    return iou_score, pixel_accuracy_\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = T.max(box1_x1, box2_x1)\n",
    "    y1 = T.max(box1_y1, box2_y1)\n",
    "    x2 = T.min(box1_x2, box2_x2)\n",
    "    y2 = T.min(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they do not intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "class MTL_VAL(Dataset):\n",
    "    def __init__(self, filename=None, input_size=(640, 480), output_size=(320, 240), n_classes=15):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.n_classes = n_classes\n",
    "        self.max_objs = 240\n",
    "        self.gaussian_iou = 0.7\n",
    "        self.dataset = pd.read_csv(self.filename)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size_x = self.input_size[0]\n",
    "        self.input_size_y = self.input_size[1]\n",
    "        self.MODEL_SCALE = self.input_size[0]//self.output_size[0]\n",
    "        self.preprocess = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.resize1 = transforms.Compose([transforms.Resize(self.input_size)])\n",
    "        self.resize2 = transforms.Compose([transforms.Resize(self.output_size)])\n",
    "\n",
    "\n",
    "    def __len__(self): return len(self.dataset)\n",
    "    \n",
    "    def __getitem_internal__(self, idx, preprocess=True):\n",
    "        target = self.dataset.iloc[idx]\n",
    "        rgb_image = cv2.imread(target[\"Path\"])\n",
    "        height, width, channels = rgb_image.shape\n",
    "        rgb_image = cv2.resize(rgb_image,self.input_size)\n",
    "        obj = make_hm_regr(target,width,height,self.n_classes,self.input_size_x,self.input_size_y,self.MODEL_SCALE,self.max_objs,self.gaussian_iou)\n",
    "        seg_mask = np.load(target[\"Seg_Path\"])\n",
    "        depth_image = np.load(target[\"Depth_path\"])\n",
    "        depth_image = cv2.resize(depth_image,self.output_size)\n",
    "        seg_mask = cv2.resize(seg_mask,self.output_size)\n",
    "        one_hot_segmask = to_one_hot(seg_mask)\n",
    "        boxes = literal_eval(target[\"bbox\"])\n",
    "        b = []\n",
    "        classes = {\"bicycle\":0,\"bus\":1,\"traffic sign\":2,\"train\":3,\"motorcycle\":4,\"car\":5,\"traffic light\":6,\"person\":7,\"vehicle fallback\":8,\"truck\":9,\"autorickshaw\":10,\"animal\":11,\"caravan\":12,\"rider\":13,\"trailer\":14}\n",
    "        for i,a in enumerate(boxes):\n",
    "            box_ = a[\"bbox\"]\n",
    "            x_scale = 640 / width\n",
    "            y_scale = 480 / height\n",
    "            x1 = int(box_[0]*x_scale)\n",
    "            y1 = int(box_[1]*y_scale)\n",
    "            x2 = int(box_[2]*x_scale)\n",
    "            y2 = int(box_[3]*y_scale)\n",
    "            b.append([x1,y1,x2,y2,classes[a[\"label\"]]])\n",
    "        if preprocess:\n",
    "            rgb_image = self.preprocess(np.array(rgb_image))\n",
    "            one_hot_segmask = transforms.ToTensor()(np.array(one_hot_segmask))\n",
    "            depth_image = transforms.ToTensor()(np.array(depth_image))\n",
    "        else:\n",
    "            rgb_image = transforms.ToTensor()(np.array(rgb_image))\n",
    "            one_hot_segmask = transforms.ToTensor()(np.array(one_hot_segmask))\n",
    "            depth_image = transforms.ToTensor()(np.array(depth_image))\n",
    "            seg_mask = transforms.ToTensor()(np.array(seg_mask))\n",
    "        return (rgb_image,seg_mask,one_hot_segmask,depth_image, b)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__getitem_internal__(idx, True)\n",
    "    \n",
    "    def raw(self, idx):\n",
    "        return self.__getitem_internal__(idx, False)\n",
    "\n",
    "test_dataloader = MTL_VAL(\"/home/b170007ec/Programs/MTL/DSD_MTL/Dataset/val_dataset.csv\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from mean_average_precision import MetricBuilder\n",
    "metric_fn = MetricBuilder.build_evaluation_metric(\"map_2d\", async_mode=True, num_classes=15)\n",
    "A1 = 0\n",
    "A2 = 0\n",
    "A3 = 0\n",
    "ABS_REL = 0\n",
    "RMSE = 0\n",
    "LOG_10 = 0\n",
    "IOU = 0\n",
    "detection_iou = []\n",
    "PIXCEL_ACC = 0\n",
    "model.eval()\n",
    "i = 0\n",
    "for k in tqdm(range(100)):\n",
    "    pred = []\n",
    "    true = []\n",
    "    rgb,seg_mask,one_hot_segmask,depth, obj = test_dataloader[k]\n",
    "    rgb = T.unsqueeze(rgb, 0)\n",
    "    rgb = rgb.to(device)\n",
    "    y_pred = model(rgb)\n",
    "    y_pred_ = F.softmax(y_pred[0],dim=1)\n",
    "    pseg = T.squeeze(y_pred_,0)\n",
    "    pseg = T.argmax(pseg, dim=0)\n",
    "    pdepth = T.squeeze(y_pred[1],0)\n",
    "    pdepth = pdepth.permute(1,2,0)\n",
    "    pdepth = pdepth.reshape(240,320)\n",
    "    hmap, regs, w_h_ = zip(*y_pred[2])\n",
    "    pred_detections = ctdet_decode(hmap[0].detach(),regs[0].detach(),w_h_[0].detach())\n",
    "    a1, a2, a3, abs_rel, rmse, log_10 = compute_errors(depth.numpy(), pdepth.cpu().detach().numpy())\n",
    "    iou,pixel_accuracy_ = IOU_SCORE(pseg.cpu().detach().numpy(), seg_mask)\n",
    "    for o in obj:\n",
    "        true.append([o[0],o[1],o[2],o[3],o[4],0,0])\n",
    "    for p in pred_detections:\n",
    "        pred.append([int(p[0]-(p[2]/2)),int(p[1]-(p[3]/2)),int(p[0]+(p[2]/2)),int(p[1]+(p[3]/2)),p[5],p[4]])\n",
    "    for u in pred:\n",
    "        best_iou = 0\n",
    "        for v in true:\n",
    "            if u[4] == v[4]:\n",
    "                iou_ = intersection_over_union(\n",
    "                    T.tensor(u[:4]),\n",
    "                    T.tensor(v[:4]),\n",
    "                    box_format=\"midpoint\",\n",
    "                )\n",
    "                if iou_ > best_iou:\n",
    "                    best_iou = iou_\n",
    "        detection_iou.append(best_iou)\n",
    "    metric_fn.add(np.array(pred), np.array(true))\n",
    "    A1 += a1\n",
    "    A2 += a2\n",
    "    A3 += a3\n",
    "    ABS_REL += abs_rel\n",
    "    RMSE += rmse\n",
    "    LOG_10 += log_10\n",
    "    IOU += iou\n",
    "    PIXCEL_ACC+=pixel_accuracy_\n",
    "    i+=1\n",
    "    \n",
    "A1 /= i\n",
    "A2 /= i\n",
    "A3 /= i\n",
    "ABS_REL /= i\n",
    "RMSE /= i\n",
    "LOG_10 /= i\n",
    "IOU /= i\n",
    "PIXCEL_ACC /= i\n",
    "seg_metric_line = r'''Segmentation :     IOU :{}\n",
    "                        Pixcel Acc :{}'''\n",
    "depth_metric_line = r'''Depth Estimation : A1 :{} \n",
    "                   A2 :{} \n",
    "                   A3 :{} \n",
    "                   ABS_REL :{} \n",
    "                   RMSE :{} \n",
    "                   LOG_10 :{}'''\n",
    "print(seg_metric_line.format(IOU,PIXCEL_ACC))\n",
    "print(depth_metric_line.format(A1,A2,A3,ABS_REL,RMSE,LOG_10))\n",
    "print(f\"VOC PASCAL mAP in all points: {metric_fn.value(iou_thresholds=0.5)['mAP']}\")\n",
    "print(\"Detection IOU :\",sum(detection_iou)/len(detection_iou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as T\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# def mean_average_precision(\n",
    "#     pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=15\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Calculates mean average precision \n",
    "#     Parameters:\n",
    "#         pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "#         specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "#         true_boxes (list): Similar as pred_boxes except all the correct ones \n",
    "#         iou_threshold (float): threshold where predicted bboxes is correct\n",
    "#         box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "#         num_classes (int): number of classes\n",
    "#     Returns:\n",
    "#         float: mAP value across all classes given a specific IoU threshold \n",
    "#     \"\"\"\n",
    "\n",
    "#     # list storing all AP for respective classes\n",
    "#     average_precisions = []\n",
    "#     average_iou = []\n",
    "#     # used for numerical stability later on\n",
    "#     epsilon = 1e-6\n",
    "\n",
    "#     for c in range(num_classes):\n",
    "#         detections = []\n",
    "#         ground_truths = []\n",
    "\n",
    "#         # Go through all predictions and targets,\n",
    "#         # and only add the ones that belong to the\n",
    "#         # current class c\n",
    "#         for detection in pred_boxes:\n",
    "#             if detection[1] == c:\n",
    "#                 detections.append(detection)\n",
    "\n",
    "#         for true_box in true_boxes:\n",
    "#             if true_box[1] == c:\n",
    "#                 ground_truths.append(true_box)\n",
    "\n",
    "#         # find the amount of bboxes for each training example\n",
    "#         # Counter here finds how many ground truth bboxes we get\n",
    "#         # for each training example, so let's say img 0 has 3,\n",
    "#         # img 1 has 5 then we will obtain a dictionary with:\n",
    "#         # amount_bboxes = {0:3, 1:5}\n",
    "#         amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "#         # We then go through each key, val in this dictionary\n",
    "#         # and convert to the following (w.r.t same example):\n",
    "#         # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "#         for key, val in amount_bboxes.items():\n",
    "#             amount_bboxes[key] = T.zeros(val)\n",
    "\n",
    "#         # sort by box probabilities which is index 2\n",
    "#         detections.sort(key=lambda x: x[2], reverse=True)\n",
    "#         TP = T.zeros((len(detections)))\n",
    "#         FP = T.zeros((len(detections)))\n",
    "#         total_true_bboxes = len(ground_truths)\n",
    "        \n",
    "#         # If none exists for this class then we can safely skip\n",
    "#         if total_true_bboxes == 0:\n",
    "#             continue\n",
    "\n",
    "#         for detection_idx, detection in enumerate(detections):\n",
    "#             # Only take out the ground_truths that have the same\n",
    "#             # training idx as detection\n",
    "#             ground_truth_img = [\n",
    "#                 bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "#             ]\n",
    "\n",
    "#             num_gts = len(ground_truth_img)\n",
    "#             best_iou = 0\n",
    "\n",
    "#             for idx, gt in enumerate(ground_truth_img):\n",
    "#                 iou = intersection_over_union(\n",
    "#                     T.tensor(detection[3:]),\n",
    "#                     T.tensor(gt[3:]),\n",
    "#                     box_format=box_format,\n",
    "#                 )\n",
    "\n",
    "#                 if iou > best_iou:\n",
    "#                     best_iou = iou\n",
    "#                     best_gt_idx = idx\n",
    "#             average_iou.append(best_iou)\n",
    "#             if best_iou > iou_threshold:\n",
    "#                 # only detect ground truth detection once\n",
    "#                 if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "#                     # true positive and add this bounding box to seen\n",
    "#                     TP[detection_idx] = 1\n",
    "#                     amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "#                 else:\n",
    "#                     FP[detection_idx] = 1\n",
    "\n",
    "#             # if IOU is lower then the detection is a false positive\n",
    "#             else:\n",
    "#                 FP[detection_idx] = 1\n",
    "\n",
    "#         TP_cumsum = T.cumsum(TP, dim=0)\n",
    "#         FP_cumsum = T.cumsum(FP, dim=0)\n",
    "#         recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "#         precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "#         precisions = T.cat((T.tensor([1]), precisions))\n",
    "#         recalls = T.cat((T.tensor([0]), recalls))\n",
    "#         # torch.trapz for numerical integration\n",
    "#         average_precisions.append(T.trapz(precisions, recalls))\n",
    "#     print(average_iou)\n",
    "#     return sum(average_precisions) / len(average_precisions), sum(average_iou)/len(average_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [03:25<00:00,  9.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# # choose codec according to format needed\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "# video=cv2.VideoWriter('result_v2.mp4', fourcc, 30,(2000,1500))\n",
    "# l = os.listdir(\"/home/b170007ec/Programs/MTL/DSD_MTL/video2\")\n",
    "# for j in tqdm(range(len(l))):\n",
    "#     img = cv2.imread(\"/home/b170007ec/Programs/MTL/DSD_MTL/video2/Predictions_{}.png\".format(j))\n",
    "#     video.write(img)\n",
    "\n",
    "# video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelA = MTL_Model(device = \"cuda\")\n",
    "# modelA_dict = modelA.state_dict()\n",
    "# modelB = MTL_ModelB(device = \"cuda\")\n",
    "# modelB.load_state_dict(T.load(\"/home/b170007ec/Programs/MTL/Bhanu/Models_SDD/model-1.713353544473648.pth\",map_location=T.device('cuda')))\n",
    "# modelB_dict = modelB.state_dict()\n",
    "# pretrained_dict = modelB_dict\n",
    "# model_dict = modelA_dict\n",
    "# # 1. filter out unnecessary keys\n",
    "# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict)\n",
    "# # 3. load the new state dict\n",
    "# modelA.load_state_dict(model_dict)\n",
    "# T.save(modelA.state_dict(), '/home/b170007ec/Programs/MTL/VQ_MTL/Models/modelvq-base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# fps = 30\n",
    "# preprocess = transforms.Compose([\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#         ])\n",
    "# video_capture = cv2.VideoCapture(\"/home/b170007ec/Programs/MTL/DSD_MTL/video.mp4\")\n",
    "# size=(1280,960)\n",
    "# frames = 0\n",
    "# model.eval()\n",
    "# while(video_capture.isOpened()):\n",
    "#     ret, frame = video_capture.read()\n",
    "#     if frames == 110:\n",
    "#         rgb_raw = cv2.resize(frame,(640, 480))\n",
    "#         rgb = preprocess(rgb_raw)\n",
    "#         rgb = T.unsqueeze(rgb, 0)\n",
    "#         rgb = rgb.to(device)\n",
    "#         y_pred = model(rgb)\n",
    "#         y_pred_ = F.softmax(y_pred[0],dim=1)\n",
    "#         pseg = T.squeeze(y_pred_,0)\n",
    "#         pseg = T.argmax(pseg, dim=0)\n",
    "#         pseg = labels_to_cityscapes_palette(pseg.cpu().detach().numpy())\n",
    "#         pdepth = T.squeeze(y_pred[1],0)\n",
    "#         pdepth = pdepth.permute(1,2,0)\n",
    "#         pdepth = pdepth.reshape(240,320)\n",
    "\n",
    "#         hmap, regs, w_h_ = zip(*y_pred[2])\n",
    "#         d = showbox(rgb_raw, hmap[0].detach(), regs[0].detach(),w_h_[0].detach())\n",
    "#         fig = plt.figure(figsize =(8, 8))\n",
    "#         plt.imshow(rgb_raw)\n",
    "#         plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "#                 labelbottom = False, bottom = False)\n",
    "#         plt.savefig(\"rgb.png\")\n",
    "#         plt.show()\n",
    "#         fig = plt.figure(figsize =(8, 8))\n",
    "#         plt.imshow(pseg)\n",
    "#         plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "#                 labelbottom = False, bottom = False)\n",
    "#         plt.savefig(\"seg.png\")\n",
    "#         plt.show()\n",
    "#         fig = plt.figure(figsize =(8, 8))\n",
    "#         plt.imshow(pdepth.cpu().detach().numpy(),cmap='magma')\n",
    "#         plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "#                 labelbottom = False, bottom = False)\n",
    "#         plt.savefig(\"dep.png\")\n",
    "#         plt.show()\n",
    "#         fig = plt.figure(figsize =(8, 8))\n",
    "#         plt.imshow(d)\n",
    "#         plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "#                 labelbottom = False, bottom = False)\n",
    "#         plt.savefig(\"det.png\")\n",
    "#         plt.show()\n",
    "#         break\n",
    "#     frames += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statistics\n",
    "  \n",
    "# def get_model_scores(pred_boxes):\n",
    "#     \"\"\"Creates a dictionary of from model_scores to image ids.\n",
    "#     Args:\n",
    "#         pred_boxes (dict): dict of dicts of 'boxes' and 'scores'\n",
    "#     Returns:\n",
    "#         dict: keys are model_scores and values are image ids (usually filenames)\n",
    "#     \"\"\"\n",
    "#     model_score={}\n",
    "#     for img_id, val in pred_boxes.items():\n",
    "#         for score in val['scores']:\n",
    "#             if score not in model_score.keys():\n",
    "#                 model_score[score]=[img_id]\n",
    "#             else:\n",
    "#                 model_score[score].append(img_id)\n",
    "#     return model_score\n",
    "\n",
    "# def calc_iou( gt_bbox, pred_bbox):\n",
    "#     x_topleft_gt, y_topleft_gt, x_bottomright_gt, y_bottomright_gt= gt_bbox[0],gt_bbox[1],gt_bbox[0]+gt_bbox[2],gt_bbox[1]+gt_bbox[3]\n",
    "#     x_topleft_p, y_topleft_p, x_bottomright_p, y_bottomright_p= pred_bbox[0],pred_bbox[1],pred_bbox[0]+pred_bbox[2],pred_bbox[1]+pred_bbox[3]\n",
    "    \n",
    "#     if (x_topleft_gt > x_bottomright_gt) or (y_topleft_gt> y_bottomright_gt):\n",
    "#         raise AssertionError(\"Ground Truth Bounding Box is not correct\")\n",
    "#     if (x_topleft_p > x_bottomright_p) or (y_topleft_p> y_bottomright_p):\n",
    "#         raise AssertionError(\"Predicted Bounding Box is not correct\",x_topleft_p, x_bottomright_p,y_topleft_p,y_bottomright_gt)\n",
    "        \n",
    "#     #if the GT bbox and predcited BBox do not overlap then iou=0\n",
    "#     if(x_bottomright_gt< x_topleft_p):\n",
    "#         # If bottom right of x-coordinate  GT  bbox is less than or above the top left of x coordinate of  the predicted BBox\n",
    "#         return 0.0\n",
    "#     if(y_bottomright_gt< y_topleft_p):  # If bottom right of y-coordinate  GT  bbox is less than or above the top left of y coordinate of  the predicted BBox\n",
    "#         return 0.0\n",
    "#     if(x_topleft_gt> x_bottomright_p): # If bottom right of x-coordinate  GT  bbox is greater than or below the bottom right  of x coordinate of  the predcited BBox\n",
    "#         return 0.0\n",
    "#     if(y_topleft_gt> y_bottomright_p): # If bottom right of y-coordinate  GT  bbox is greater than or below the bottom right  of y coordinate of  the predcited BBox\n",
    "#         return 0.0\n",
    "    \n",
    "#     GT_bbox_area = (x_bottomright_gt -  x_topleft_gt + 1) * (  y_bottomright_gt -y_topleft_gt + 1)\n",
    "#     Pred_bbox_area =(x_bottomright_p - x_topleft_p + 1 ) * ( y_bottomright_p -y_topleft_p + 1)\n",
    "    \n",
    "#     x_top_left =np.max([x_topleft_gt, x_topleft_p])\n",
    "#     y_top_left = np.max([y_topleft_gt, y_topleft_p])\n",
    "#     x_bottom_right = np.min([x_bottomright_gt, x_bottomright_p])\n",
    "#     y_bottom_right = np.min([y_bottomright_gt, y_bottomright_p])\n",
    "    \n",
    "#     intersection_area = (x_bottom_right- x_top_left + 1) * (y_bottom_right-y_top_left  + 1)\n",
    "    \n",
    "#     union_area = (GT_bbox_area + Pred_bbox_area - intersection_area)\n",
    "   \n",
    "#     return intersection_area/union_area\n",
    "\n",
    "# def calc_precision_recall(image_results):\n",
    "#     \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "#     Args:\n",
    "#         gt_boxes (list of list of floats): list of locations of ground truth\n",
    "#             objects as [xmin, ymin, xmax, ymax]\n",
    "#         pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "#             and 'scores'\n",
    "#         iou_thr (float): value of IoU to consider as threshold for a\n",
    "#             true prediction.\n",
    "#     Returns:\n",
    "#         dict: true positives (int), false positives (int), false negatives (int)\n",
    "#     \"\"\"\n",
    "#     true_positive=0\n",
    "#     false_positive=0\n",
    "#     false_negative=0\n",
    "#     for img_id, res in image_results.items():\n",
    "#         true_positive +=res['true_positive']\n",
    "#         false_positive += res['false_positive']\n",
    "#         false_negative += res['false_negative']\n",
    "#         try:\n",
    "#             precision = true_positive/(true_positive+ false_positive)\n",
    "#         except ZeroDivisionError:\n",
    "#             precision=0.0\n",
    "#         try:\n",
    "#             recall = true_positive/(true_positive + false_negative)\n",
    "#         except ZeroDivisionError:\n",
    "#             recall=0.0\n",
    "#     return (precision, recall)\n",
    "\n",
    "# def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "#     \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "#     Args:\n",
    "#         gt_boxes (list of list of floats): list of locations of ground truth\n",
    "#             objects as [xmin, ymin, xmax, ymax]\n",
    "#         pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "#             and 'scores'\n",
    "#         iou_thr (float): value of IoU to consider as threshold for a\n",
    "#             true prediction.\n",
    "#     Returns:\n",
    "#         dict: true positives (int), false positives (int), false negatives (int)\n",
    "#     \"\"\"\n",
    "#     all_pred_indices= range(len(pred_boxes))\n",
    "#     all_gt_indices=range(len(gt_boxes))\n",
    "#     if len(all_pred_indices)==0:\n",
    "#         tp=0\n",
    "#         fp=0\n",
    "#         fn=0\n",
    "#         return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "#     if len(all_gt_indices)==0:\n",
    "#         tp=0\n",
    "#         fp=0\n",
    "#         fn=0\n",
    "#         return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    \n",
    "#     gt_idx_thr=[]\n",
    "#     pred_idx_thr=[]\n",
    "#     ious=[]\n",
    "#     for ipb, pred_box in enumerate(pred_boxes):\n",
    "#         for igb, gt_box in enumerate(gt_boxes):\n",
    "#             iou= calc_iou(gt_box, pred_box)\n",
    "            \n",
    "#             if iou >iou_thr:\n",
    "#                 gt_idx_thr.append(igb)\n",
    "#                 pred_idx_thr.append(ipb)\n",
    "#                 ious.append(iou)\n",
    "#     iou_sort = np.argsort(ious)[::1]\n",
    "#     print(statistics.mean(ious))\n",
    "#     if len(iou_sort)==0:\n",
    "#         tp=0\n",
    "#         fp=0\n",
    "#         fn=0\n",
    "#         return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "#     else:\n",
    "#         gt_match_idx=[]\n",
    "#         pred_match_idx=[]\n",
    "#         for idx in iou_sort:\n",
    "#             gt_idx=gt_idx_thr[idx]\n",
    "#             pr_idx= pred_idx_thr[idx]\n",
    "#             # If the boxes are unmatched, add them to matches\n",
    "#             if(gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "#                 gt_match_idx.append(gt_idx)\n",
    "#                 pred_match_idx.append(pr_idx)\n",
    "#         tp= len(gt_match_idx)\n",
    "#         fp= len(pred_boxes) - len(pred_match_idx)\n",
    "#         fn = len(gt_boxes) - len(gt_match_idx)\n",
    "#     return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}\n",
    "\n",
    "\n",
    "\n",
    "# def  get_avg_precision_at_iou(gt_boxes, pred_bb, iou_thr=0.5):\n",
    "#     model_scores = get_model_scores(pred_bb)\n",
    "#     sorted_model_scores= sorted(model_scores.keys())\n",
    "# # Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
    "#     for img_id in pred_bb.keys():\n",
    "        \n",
    "#         arg_sort = np.argsort(pred_bb[img_id]['scores'])\n",
    "#         pred_bb[img_id]['scores'] = np.array(pred_bb[img_id]['scores'])[arg_sort].tolist()\n",
    "#         pred_bb[img_id]['boxes'] = np.array(pred_bb[img_id]['boxes'])[arg_sort].tolist()\n",
    "    \n",
    "#     pred_boxes_pruned = deepcopy(pred_bb)\n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     model_thrs = []\n",
    "#     img_results = {}\n",
    "# # Loop over model score thresholds and calculate precision, recall\n",
    "#     for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
    "#             # On first iteration, define img_results for the first time:\n",
    "#         print(\"Mode score : \", model_score_thr)\n",
    "#         img_ids = gt_boxes.keys() if ithr == 0 else model_scores[model_score_thr]\n",
    "#         for img_id in img_ids:\n",
    "#             gt_boxes_img = gt_boxes[img_id]\n",
    "#             box_scores = pred_boxes_pruned[img_id]['scores']\n",
    "#             start_idx = 0\n",
    "#             for score in box_scores:\n",
    "#                 if score <= model_score_thr:\n",
    "#                     pred_boxes_pruned[img_id]\n",
    "#                     start_idx += 1\n",
    "#                 else:\n",
    "#                     break \n",
    "#             # Remove boxes, scores of lower than threshold scores:\n",
    "#             pred_boxes_pruned[img_id]['scores']= pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
    "#             pred_boxes_pruned[img_id]['boxes']= pred_boxes_pruned[img_id]['boxes'][start_idx:]\n",
    "# # Recalculate image results for this image\n",
    "#             print(img_id)\n",
    "#             img_results[img_id] = get_single_image_results(gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr=0.5)\n",
    "# # calculate precision and recall\n",
    "#         prec, rec = calc_precision_recall(img_results)\n",
    "#         precisions.append(prec)\n",
    "#         recalls.append(rec)\n",
    "#         model_thrs.append(model_score_thr)\n",
    "#     precisions = np.array(precisions)\n",
    "#     recalls = np.array(recalls)\n",
    "#     prec_at_rec = []\n",
    "#     for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "#         try:\n",
    "#             args= np.argwhere(recalls>recall_level).flatten()\n",
    "#             prec= max(precisions[args])\n",
    "#             print(recalls,\"Recall\")\n",
    "#             print(      recall_level,\"Recall Level\")\n",
    "#             print(       args, \"Args\")\n",
    "#             print(       prec, \"precision\")\n",
    "#         except ValueError:\n",
    "#             prec=0.0\n",
    "#         prec_at_rec.append(prec)\n",
    "#     avg_prec = np.mean(prec_at_rec) \n",
    "#     return {\n",
    "#         'avg_prec': avg_prec,\n",
    "#         'precisions': precisions,\n",
    "#         'recalls': recalls,\n",
    "#         'model_thrs': model_thrs}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
